{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#english to French SEQ2SEQ model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\Users\\santh_000\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from collections import Counter\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import zipfile\n",
    "import sys\n",
    "import urllib.request\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.utils import plot_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.activations import softmax\n",
    "from keras.layers.core import Dense, Activation, RepeatVector, Permute\n",
    "from keras.layers import Input, Embedding, Multiply, Concatenate, Lambda\n",
    "from keras.layers.wrappers import TimeDistributed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 100\n",
    "HIDDEN_UNITS = 256\n",
    "NUM_SAMPLES = 10000\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "EMBEDDING_SIZE = 100\n",
    "DATA = 'fra.txt'#loading data from default work directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tar_count = Counter()\n",
    "\n",
    "GLOVE_MODEL = \"glove.6B.100d.txt\"\n",
    "WEIGHT_FILE_PATH = 'eng-to-fr-glove-weights.h5'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove():\n",
    "    \n",
    "    _word2em = {}\n",
    "    file = open(GLOVE_MODEL, mode='r', encoding='utf8')\n",
    "    for line in file:\n",
    "        words = line.strip().split()\n",
    "        word = words[0]\n",
    "        embeds = np.array(words[1:], dtype=np.float32)\n",
    "        _word2em[word] = embeds\n",
    "    file.close()\n",
    "    return _word2em"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2em = load_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(DATA, 'r', encoding='utf8').read().split('\\n')\n",
    "for line in lines[: len(lines)-1]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    input_words = [w for w in nltk.word_tokenize(input_text.lower())]\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    for char in target_text:\n",
    "        tar_count[char] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_word2idx = dict()\n",
    "\n",
    "for idx, word in enumerate(tar_count.most_common(MAX_VOCAB_SIZE)):\n",
    "    #print(word)\n",
    "    target_word2idx[word[0]] = idx\n",
    "\n",
    "target_idx2word = dict([(idx, word) for word, idx in target_word2idx.items()])\n",
    "\n",
    "num_decoder_tokens = len(target_idx2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_emb = np.random.randn(EMBEDDING_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "\n",
    "input_texts_word2em = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encoder word2index input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = open(DATA, 'r', encoding='utf8').read().split('\\n')\n",
    "for line in lines[: min(NUM_SAMPLES, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_words = [w for w in nltk.word_tokenize(input_text.lower())]\n",
    "    encoder_input_wids = []\n",
    "    for w in input_words:\n",
    "        em = unknown_emb\n",
    "        if w in word2em:\n",
    "            em = word2em[w]\n",
    "        encoder_input_wids.append(em)\n",
    "    input_texts_word2em.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_text), decoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = pad_sequences(input_texts_word2em, encoder_max_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decoder word2index input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_target_data = np.zeros(shape=(NUM_SAMPLES, decoder_max_seq_length, num_decoder_tokens))\n",
    "decoder_input_data = np.zeros(shape=(NUM_SAMPLES, decoder_max_seq_length, num_decoder_tokens))\n",
    "lines = open(DATA, 'rt', encoding='utf8').read().split('\\n')\n",
    "for lineIdx, line in enumerate(lines[: min(NUM_SAMPLES, len(lines)-1)]):\n",
    "    _, target = line.split('\\t')\n",
    "    target = '\\t' + target + '\\n'\n",
    "    for idx, char in enumerate(target):\n",
    "        if char in target_word2idx:\n",
    "            w2idx = target_word2idx[char]\n",
    "            decoder_input_data[lineIdx, idx, w2idx] = 1\n",
    "            if idx > 0:\n",
    "                decoder_target_data[lineIdx, idx-1, w2idx] = 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = dict()\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining Encoder- Decoder Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = Input(shape=(None, EMBEDDING_SIZE), name='encoder_inputs')\n",
    "encoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, name='encoder_lstm')\n",
    "encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "encoder_states = [encoder_state_h, encoder_state_c]\n",
    "\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_inputs')\n",
    "decoder_lstm = LSTM(units=HIDDEN_UNITS, return_state=True, return_sequences=True, name='decoder_lstm')\n",
    "decoder_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_inputs,\n",
    "                                                                 initial_state=encoder_states)\n",
    "decoder_dense = Dense(units=num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7000 samples, validate on 3000 samples\n",
      "Epoch 1/100\n",
      "7000/7000 [==============================] - 86s 12ms/step - loss: 0.9723 - val_loss: 1.0034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\santh_000\\Anaconda3\\envs\\tfp3.6\\lib\\site-packages\\keras\\engine\\topology.py:2368: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100\n",
      "7000/7000 [==============================] - 78s 11ms/step - loss: 0.7787 - val_loss: 0.8206\n",
      "Epoch 3/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.6341 - val_loss: 0.7187\n",
      "Epoch 4/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.5636 - val_loss: 0.6648\n",
      "Epoch 5/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.5230 - val_loss: 0.6357\n",
      "Epoch 6/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.4947 - val_loss: 0.6140\n",
      "Epoch 7/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.4728 - val_loss: 0.5887\n",
      "Epoch 8/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.4525 - val_loss: 0.5781\n",
      "Epoch 9/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.4347 - val_loss: 0.5599\n",
      "Epoch 10/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.4180 - val_loss: 0.5511\n",
      "Epoch 11/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.4029 - val_loss: 0.5402\n",
      "Epoch 12/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.3900 - val_loss: 0.5306\n",
      "Epoch 13/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.3763 - val_loss: 0.5245\n",
      "Epoch 14/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.3642 - val_loss: 0.5163\n",
      "Epoch 15/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.3518 - val_loss: 0.5117\n",
      "Epoch 16/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.3410 - val_loss: 0.5067\n",
      "Epoch 17/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.3308 - val_loss: 0.5042\n",
      "Epoch 18/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.3199 - val_loss: 0.5022\n",
      "Epoch 19/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.3100 - val_loss: 0.5010\n",
      "Epoch 20/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.3008 - val_loss: 0.4987\n",
      "Epoch 21/100\n",
      "7000/7000 [==============================] - 74s 11ms/step - loss: 0.2908 - val_loss: 0.4951\n",
      "Epoch 22/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.2814 - val_loss: 0.4977\n",
      "Epoch 23/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.2723 - val_loss: 0.4983\n",
      "Epoch 24/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.2636 - val_loss: 0.4995\n",
      "Epoch 25/100\n",
      "7000/7000 [==============================] - 80s 11ms/step - loss: 0.2547 - val_loss: 0.5019\n",
      "Epoch 26/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.2470 - val_loss: 0.5075\n",
      "Epoch 27/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.2384 - val_loss: 0.5091\n",
      "Epoch 28/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.2310 - val_loss: 0.5093\n",
      "Epoch 29/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.2233 - val_loss: 0.5189\n",
      "Epoch 30/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.2156 - val_loss: 0.5149\n",
      "Epoch 31/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.2082 - val_loss: 0.5229\n",
      "Epoch 32/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.2011 - val_loss: 0.5266\n",
      "Epoch 33/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.1941 - val_loss: 0.5286\n",
      "Epoch 34/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1874 - val_loss: 0.5358\n",
      "Epoch 35/100\n",
      "7000/7000 [==============================] - 88s 13ms/step - loss: 0.1810 - val_loss: 0.5430\n",
      "Epoch 36/100\n",
      "7000/7000 [==============================] - 91s 13ms/step - loss: 0.1753 - val_loss: 0.5486\n",
      "Epoch 37/100\n",
      "7000/7000 [==============================] - 87s 12ms/step - loss: 0.1686 - val_loss: 0.5484\n",
      "Epoch 38/100\n",
      "7000/7000 [==============================] - 78s 11ms/step - loss: 0.1634 - val_loss: 0.5603\n",
      "Epoch 39/100\n",
      "7000/7000 [==============================] - 86s 12ms/step - loss: 0.1577 - val_loss: 0.5650\n",
      "Epoch 40/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1510 - val_loss: 0.5730\n",
      "Epoch 41/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1466 - val_loss: 0.5740\n",
      "Epoch 42/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1414 - val_loss: 0.5833\n",
      "Epoch 43/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.1364 - val_loss: 0.5857\n",
      "Epoch 44/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1316 - val_loss: 0.5966\n",
      "Epoch 45/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.1268 - val_loss: 0.6084\n",
      "Epoch 46/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.1226 - val_loss: 0.6156\n",
      "Epoch 47/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1190 - val_loss: 0.6162\n",
      "Epoch 48/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1141 - val_loss: 0.6246\n",
      "Epoch 49/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.1100 - val_loss: 0.6401\n",
      "Epoch 50/100\n",
      "7000/7000 [==============================] - 83s 12ms/step - loss: 0.1067 - val_loss: 0.6456\n",
      "Epoch 51/100\n",
      "7000/7000 [==============================] - 80s 11ms/step - loss: 0.1038 - val_loss: 0.6490\n",
      "Epoch 52/100\n",
      "7000/7000 [==============================] - 78s 11ms/step - loss: 0.0988 - val_loss: 0.6604\n",
      "Epoch 53/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0967 - val_loss: 0.6662\n",
      "Epoch 54/100\n",
      "7000/7000 [==============================] - 85s 12ms/step - loss: 0.0922 - val_loss: 0.6760\n",
      "Epoch 55/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0899 - val_loss: 0.6774\n",
      "Epoch 56/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0879 - val_loss: 0.6893\n",
      "Epoch 57/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0844 - val_loss: 0.6919\n",
      "Epoch 58/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0823 - val_loss: 0.7017\n",
      "Epoch 59/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0794 - val_loss: 0.7139\n",
      "Epoch 60/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0763 - val_loss: 0.7184\n",
      "Epoch 61/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0744 - val_loss: 0.7257\n",
      "Epoch 62/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0714 - val_loss: 0.7335\n",
      "Epoch 63/100\n",
      "7000/7000 [==============================] - 78s 11ms/step - loss: 0.0708 - val_loss: 0.7422\n",
      "Epoch 64/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0688 - val_loss: 0.7498\n",
      "Epoch 65/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0660 - val_loss: 0.7498\n",
      "Epoch 66/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0643 - val_loss: 0.7658\n",
      "Epoch 67/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0631 - val_loss: 0.7741\n",
      "Epoch 68/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0613 - val_loss: 0.7813\n",
      "Epoch 69/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0604 - val_loss: 0.7804\n",
      "Epoch 70/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0583 - val_loss: 0.7906\n",
      "Epoch 71/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0565 - val_loss: 0.7929\n",
      "Epoch 72/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0554 - val_loss: 0.8070\n",
      "Epoch 73/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0543 - val_loss: 0.8119\n",
      "Epoch 74/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0523 - val_loss: 0.8166\n",
      "Epoch 75/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0516 - val_loss: 0.8221\n",
      "Epoch 76/100\n",
      "7000/7000 [==============================] - 82s 12ms/step - loss: 0.0501 - val_loss: 0.8243\n",
      "Epoch 77/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0495 - val_loss: 0.8286\n",
      "Epoch 78/100\n",
      "7000/7000 [==============================] - 84s 12ms/step - loss: 0.0488 - val_loss: 0.8341\n",
      "Epoch 79/100\n",
      "7000/7000 [==============================] - 80s 11ms/step - loss: 0.0475 - val_loss: 0.8483\n",
      "Epoch 80/100\n",
      "7000/7000 [==============================] - 78s 11ms/step - loss: 0.0464 - val_loss: 0.8511\n",
      "Epoch 81/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0462 - val_loss: 0.8535\n",
      "Epoch 82/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0457 - val_loss: 0.8565\n",
      "Epoch 83/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0456 - val_loss: 0.8700\n",
      "Epoch 84/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0437 - val_loss: 0.8637\n",
      "Epoch 85/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0426 - val_loss: 0.8698\n",
      "Epoch 86/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0425 - val_loss: 0.8738\n",
      "Epoch 87/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0421 - val_loss: 0.8703\n",
      "Epoch 88/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0411 - val_loss: 0.8833\n",
      "Epoch 89/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0404 - val_loss: 0.8904\n",
      "Epoch 90/100\n",
      "7000/7000 [==============================] - 76s 11ms/step - loss: 0.0404 - val_loss: 0.8888\n",
      "Epoch 91/100\n",
      "7000/7000 [==============================] - 75s 11ms/step - loss: 0.0392 - val_loss: 0.8936\n",
      "Epoch 92/100\n",
      "7000/7000 [==============================] - 80s 11ms/step - loss: 0.0388 - val_loss: 0.8970\n",
      "Epoch 93/100\n",
      "7000/7000 [==============================] - 78s 11ms/step - loss: 0.0394 - val_loss: 0.8979\n",
      "Epoch 94/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0380 - val_loss: 0.9015\n",
      "Epoch 95/100\n",
      "7000/7000 [==============================] - 80s 11ms/step - loss: 0.0374 - val_loss: 0.9021\n",
      "Epoch 96/100\n",
      "7000/7000 [==============================] - 81s 12ms/step - loss: 0.0362 - val_loss: 0.9131\n",
      "Epoch 97/100\n",
      "7000/7000 [==============================] - 90s 13ms/step - loss: 0.0359 - val_loss: 0.9143\n",
      "Epoch 98/100\n",
      "7000/7000 [==============================] - 86s 12ms/step - loss: 0.0358 - val_loss: 0.9154\n",
      "Epoch 99/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0360 - val_loss: 0.9315\n",
      "Epoch 100/100\n",
      "7000/7000 [==============================] - 77s 11ms/step - loss: 0.0367 - val_loss: 0.9211\n"
     ]
    }
   ],
   "source": [
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "checkpoint = ModelCheckpoint(filepath=WEIGHT_FILE_PATH, save_best_only=True)\n",
    "model.fit([encoder_input_data, decoder_input_data], decoder_target_data, batch_size=BATCH_SIZE, epochs=NUM_EPOCHS,\n",
    "          verbose=1, validation_split=0.3, callbacks=[checkpoint])\n",
    "\n",
    "model.save_weights(WEIGHT_FILE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 100)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, None, 116)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 256), (None, 365568      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  381952      decoder_inputs[0][0]             \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 116)    29812       decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 777,332\n",
      "Trainable params: 777,332\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder inference model\n",
    "encoder_model_inf = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder inference model\n",
    "\n",
    "decoder_state_input_h = Input(shape=(HIDDEN_UNITS,))\n",
    "decoder_state_input_c = Input(shape=(HIDDEN_UNITS,))\n",
    "decoder_input_states = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "decoder_out, decoder_h, decoder_c = decoder_lstm(decoder_inputs, \n",
    "                                                 initial_state=decoder_input_states)\n",
    "\n",
    "decoder_states = [decoder_h , decoder_c]\n",
    "\n",
    "decoder_out = decoder_dense(decoder_out)\n",
    "\n",
    "decoder_model_inf = Model(inputs=[decoder_inputs] + decoder_input_states,\n",
    "                          outputs=[decoder_out] + decoder_states )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = context['encoder_max_seq_length']\n",
    "max_decoder_seq_length = context['decoder_max_seq_length']\n",
    "num_decoder_tokens = context['num_decoder_tokens']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sent(input_text):\n",
    "        input_seq = []\n",
    "        input_wids = []\n",
    "        for word in nltk.word_tokenize(input_text.lower()):\n",
    "            emb = unknown_emb\n",
    "            if word in word2em:\n",
    "                emb = word2em[word]\n",
    "            input_wids.append(emb)\n",
    "        input_seq.append(input_wids)\n",
    "        input_seq = pad_sequences(input_seq, max_encoder_seq_length)\n",
    "        states_value = encoder_model_inf.predict(input_seq)\n",
    "        target_seq = np.zeros((1, 1,num_decoder_tokens))\n",
    "        target_seq[0, 0, target_word2idx['\\t']] = 1\n",
    "        target_text = ''\n",
    "        terminated = False\n",
    "        while not terminated:\n",
    "            output_tokens, h, c = decoder_model_inf.predict([target_seq] + states_value)\n",
    "\n",
    "            sample_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "            sample_word = target_idx2word[sample_token_idx]\n",
    "            target_text += sample_word\n",
    "\n",
    "            if sample_word == '\\n' or len(target_text) >= max_decoder_seq_length:\n",
    "                terminated = True\n",
    "\n",
    "            target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "            target_seq[0, 0, sample_token_idx] = 1\n",
    "\n",
    "            states_value = [h, c]\n",
    "        return target_text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Il est malade.\n"
     ]
    }
   ],
   "source": [
    "print(predict_sent('He is sick.'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfp3.6]",
   "language": "python",
   "name": "conda-env-tfp3.6-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
