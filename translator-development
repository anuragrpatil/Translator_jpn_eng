import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

import keras
from keras import optimizers
from keras import backend as K
from keras import regularizers
from keras.models import Sequential
from keras.layers import Dense, Activation, Dropout, Flatten
from keras.layers import Embedding, Conv1D, MaxPooling1D, GlobalMaxPooling1D 
from keras.utils import plot_model
from keras.preprocessing import sequence
from keras.preprocessing.text import Tokenizer
from keras.callbacks import EarlyStopping
from keras.models import Model
from keras.layers import Input, LSTM, Dense
from keras.models import load_model

from tqdm import tqdm
from nltk.corpus import stopwords
from nltk.tokenize import RegexpTokenizer 
import os, re, csv, math, codecs
import io

print('loading word embeddings input...')
embeddings_index_input = {}
f = codecs.open('cc.en.300.vec', encoding='utf-8')
for line in tqdm(f):
    values = line.rstrip().rsplit(' ')
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index_input[word] = coefs
f.close()
print('found %s word vectors' % len(embeddings_index_input))

print('loading word embeddings target...')
embeddings_index_target = {}
f = codecs.open('cc.ja.300.vec', encoding='utf-8')
for line in tqdm(f):
    values = line.rstrip().rsplit(' ')
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index_target[word] = coefs
f.close()
print('found %s word vectors' % len(embeddings_index_target))

batch_size = 64  # Batch size for training.
epochs = 100  # Number of epochs to train for.
latent_dim = 256  # Latent dimensionality of the encoding space.
num_samples = 10000  # Number of samples to train on.

# Path to the data txt file on disk.
data_path = 'jpn-eng/jpn.txt'

# Vectorize the data.
input_texts = []
target_texts = []
input_characters = set()
target_characters = set()
with io.open(data_path, 'r', encoding='utf-8') as f:
    lines = f.read().split('\n')
for line in lines[: min(num_samples, len(lines) - 1)]:
    input_text, target_text = line.split('\t')
    # We use "tab" as the "start sequence" character
    # for the targets, and "\n" as "end sequence" character.
    target_text = '\t' + target_text + '\n'
    input_texts.append(input_text)
    target_texts.append(target_text)
 
 from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(nb_words=1000)
tokenizer.fit_on_texts(input_texts)
#sequences = tokenizer.texts_to_sequences(target_texts)

word_index_input = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index_input))
print(word_index_input)

from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences

tokenizer = Tokenizer(nb_words=1000)
tokenizer.fit_on_texts(target_texts)
sequences = tokenizer.texts_to_sequences(target_texts)

word_index_target = tokenizer.word_index
print('Found %s unique tokens.' % len(word_index_target))
print(word_index_target)

embedding_matrix_input = np.zeros((len(word_index_input) + 1, 300))
for word, i in word_index_input.items():
    embedding_vector = embeddings_index_input.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix_input[i] = embedding_vector

print(embedding_matrix_input)

embedding_matrix_target = np.zeros((len(word_index_target) + 1, 300))
for word, i in word_index_target.items():
    embedding_vector = embeddings_index_target.get(word)
    if embedding_vector is not None:
        # words not found in embedding index will be all-zeros.
        embedding_matrix_target[i] = embedding_vector

print(embedding_matrix_target)
print(embedding_matrix_target.shape[0])

max_encoder_seq_length = max([len(txt) for txt in input_texts])
max_decoder_seq_length = max([len(txt) for txt in target_texts])

# Define an input sequence and process it.
encoder_inputs = Input(shape=(None,))
x = Embedding(len(word_index_input) + 1,
                            300,
                            weights=[embedding_matrix_input],
                            input_length=max_encoder_seq_length,
                            trainable=False)(encoder_inputs)
x, state_h, state_c = LSTM(latent_dim,
                           return_state=True)(x)
encoder_states = [state_h, state_c]

# Set up the decoder, using `encoder_states` as initial state.
decoder_inputs = Input(shape=(None,))
x = Embedding(len(word_index_target) + 1,
                            300,
                            weights=[embedding_matrix_target],
                            input_length=max_decoder_seq_length,
                            trainable=False)(decoder_inputs)
x = LSTM(latent_dim, return_sequences=True)(x, initial_state=encoder_states)

decoder_outputs = Dense(300, activation='softmax')(x)

encoder_input_data = np.zeros(
    (len(input_texts),max_encoder_seq_length, 300))
decoder_input_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, 300))
decoder_target_data = np.zeros(
    (len(input_texts), max_decoder_seq_length, 300))
    
    encoder_input_data = np.zeros(
    (len(word_index_target) + 1,max_encoder_seq_length, 300))
decoder_input_data = np.zeros(
    (len(word_index_target) + 1, max_decoder_seq_length, 300))
decoder_target_data = np.zeros(
    (len(word_index_target) + 1, max_decoder_seq_length, 300))
    
    # Define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Run training
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')

model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)
# Save model
model.add(Flatten())
model.save('s2s-words.h5')

# Define the model that will turn
# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`
model = Model([encoder_inputs, decoder_inputs], decoder_outputs)

# Compile & run training
model.compile(optimizer='rmsprop', loss='categorical_crossentropy')
# Note that `decoder_target_data` needs to be one-hot encoded,
# rather than sequences of integers like `decoder_input_data`!
model.fit([encoder_input_data, decoder_input_data], decoder_target_data,
          batch_size=batch_size,
          epochs=epochs,
          validation_split=0.2)
# Save model
model.save('s2s-words.h5')
